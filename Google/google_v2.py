
import re
import os
import time
import urllib3
import requests
import traceback
import pandas as pd
from datetime import datetime
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, WebDriverException

####################################################################################### CONFIG 
API_KEY = "AIzaSyDBUcnY9yG5ZRK0WzhJQLuGW-j6BOcwBaY"
SEARCH_ENGINE_ID = "f3dc1d67c30ed47dc"
TITLE = 'title'
LINK = 'link'
SNIPPET = 'snippet' # DESCRIPTION
CONTENT = 'content'
SUMMARIZE = 'summarize'
IS_RELATED = 'related'
USEFUL_INFO = 'useful info'
CODE = ['es','fr','de','pt','th']
NUM_REQUEST = 3 # Max -> 100
RESULTS_PER_REQUEST = 3 # Google gi·ªõi h·∫°n t·ªëi ƒëa 10/l·∫ßn
####################################################################################### GLOBAL VARS
all_results = []
list_ignore = ['samsung.com', 'amazon.com', 'apple.com', 'threads.net']

start_time = time.time()
cur_time = start_time

key = "Network Unlock of Samsung smartphones"
query = 'network unlock "samsung"'
queries = {}
queries['en'] = query

####################################################################################### TRANSLATE
def get_api_url(text, from_lang, to_lang):
    return f"https://api.mymemory.translated.net/get?q={text}&langpair={from_lang}%7C{to_lang}"

def translate_using_api(text, to_lang):
    try: 
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        url = get_api_url(text, 'en', to_lang)
        response = requests.get(url, verify=False)
        data = response.json()
        response.raise_for_status()  # Raise an exception for HTTP errors
        return data['responseData']['translatedText']
    except Exception as e:
        return F"Fail to translate to {to_lang} query: {text}"

for lang in CODE:
    queries[lang] = translate_using_api(query, lang)
end_time = time.time()  # ghi l·∫°i th·ªùi gian k·∫øt th√∫c
elapsed_time = end_time - cur_time
cur_time = end_time
print(f"‚è±Ô∏è Th·ªùi gian ch·∫°y update_queries: {elapsed_time:.2f} gi√¢y")

####################################################################################### SEARCH API
def search_api(query, num_results = NUM_REQUEST, start_page = 0, date_restrict="m1"):
    url = 'https://www.googleapis.com/customsearch/v1'

    results_per_request = RESULTS_PER_REQUEST  # Google gi·ªõi h·∫°n t·ªëi ƒëa 10/l·∫ßn
    params = {
        'q'             : query,
        'key'           : API_KEY,
        'cx'            : SEARCH_ENGINE_ID,
        'num'           : results_per_request,
        'start'         : start_page,
        'dateRestrict'  : date_restrict,
    }

    for start in range(1, num_results, results_per_request):
        print(f"Finding {start} to {start + results_per_request - 1}")
        params['start'] = start
        response = requests.get(url, params=params)
        
        if response.status_code == 200:
            data = response.json()
            data_items = data.get("items", [])
            # print(f"Start: {start}, Result: {data_items}")
            if len(data_items) == 0:
                print("End result")
                break

            for item in data_items:
                if all(ignore not in item[LINK] for ignore in list_ignore):
                        # publish_date = get_publish_from_des(description)
                        all_results.append({
                             TITLE: item[TITLE], 
                             LINK: item[LINK], 
                             SNIPPET: item[SNIPPET], 
                             CONTENT: "",
                             SUMMARIZE: "",
                             IS_RELATED: "",
                             USEFUL_INFO: "",
                             })
        else:
            print(f"L·ªói {response.status_code}: {response.text}")
            break  # D·ª´ng l·∫°i n·∫øu c√≥ l·ªói

for que in queries:
    search_api(que)

end_time = time.time()  # ghi l·∫°i th·ªùi gian k·∫øt th√∫c
elapsed_time = end_time - cur_time
cur_time = end_time
print(f"‚è±Ô∏è Th·ªùi gian ch·∫°y search_api: {elapsed_time:.2f} gi√¢y")

def remove_duplicate_links(array):
    unique_arr = []
    for index, item in enumerate(array):
        check_uni = False
        for i in range(index):
            if item[LINK] == array[i][LINK]:
                check_uni = True
                break
        if check_uni == False:
            unique_arr.append(item)
    return unique_arr

all_results = remove_duplicate_links(all_results)
end_time = time.time()  # ghi l·∫°i th·ªùi gian k·∫øt th√∫c
elapsed_time = end_time - cur_time
cur_time = end_time
print(f"‚è±Ô∏è Th·ªùi gian ch·∫°y remove_duplicate_links: {elapsed_time:.2f} gi√¢y")
print(f"‚úÖ T·ªïng s·ªë links t√¨m ƒë∆∞·ª£c: {len(all_results):.2f}")


####################################################################################### GET CONTENT

def get_content():
    driver = webdriver.Chrome()
    driver.set_page_load_timeout(10)

    for item in all_results:
        clean_text = ''
        try:
            driver.get(item[LINK])
        except TimeoutException:
            print(f"[TIMEOUT] Trang load qu√° l√¢u: {item[LINK]}")
            try:
                page_source = driver.page_source
                # L·∫•y n·ªôi dung n·∫øu ƒë√£ c√≥
                soup = BeautifulSoup(page_source, "html.parser")
                for tag in soup(["script", "style"]):
                    tag.decompose()
                lines = [line.strip() for line in soup.get_text(separator="\n").splitlines() if line.strip()]
                clean_text = '\n'.join(lines)
            except Exception as e:
                traceback.print_exc()
                print(f"[FAIL] Kh√¥ng th·ªÉ l·∫•y page_source sau timeout: {item[LINK]}")
                continue  # B·ªè qua n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c g√¨
        except WebDriverException:
            traceback.print_exc()
            print(f"[ERROR] WebDriver g·∫∑p l·ªói khi truy c·∫≠p: {item[LINK]}")
            continue
        else:
            # N·∫øu kh√¥ng timeout, x·ª≠ l√Ω b√¨nh th∆∞·ªùng
            page_source = driver.page_source
            soup = BeautifulSoup(page_source, "html.parser")
            for tag in soup(["script", "style"]):
                tag.decompose()
            lines = [line.strip() for line in soup.get_text(separator="\n").splitlines() if line.strip()]
            clean_text = '\n'.join(lines)

        item[CONTENT] = clean_text

    driver.quit()
get_content()

end_time = time.time()  # ghi l·∫°i th·ªùi gian k·∫øt th√∫c
elapsed_time = end_time - cur_time
cur_time = end_time
print(f"‚è±Ô∏è Th·ªùi gian ch·∫°y update_content: {elapsed_time:.2f} gi√¢y")
    
####################################################################################### AI SUPPORT
def strip_thoughts(text):
    """
    Lo·∫°i b·ªè ph·∫ßn n·∫±m trong <think>...</think> v√† tr·∫£ v·ªÅ ph·∫ßn n·ªôi dung sau ƒë√≥.

    Args:
        text (str): Chu·ªói ƒë·∫ßu v√†o, c√≥ th·ªÉ ch·ª©a ph·∫ßn <think>...</think>

    Returns:
        str: Chu·ªói ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch, ch·ªâ gi·ªØ ph·∫ßn k·∫øt lu·∫≠n cu·ªëi c√πng.
    """
    # D√πng regex ƒë·ªÉ lo·∫°i b·ªè ƒëo·∫°n <think>...</think>
    clean_text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL | re.IGNORECASE)

    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng ƒë·∫ßu/cu·ªëi v√† d√≤ng tr·ªëng th·ª´a
    clean_text = clean_text.strip()

    return clean_text

def summarize_text(title, snippet, link, content, num_words = 50, model="gemma3:1b"):
    prompt = (
        f"B·∫°n l√† m·ªôt chuy√™n gia ƒë·ªçc hi·ªÉu, ph√¢n t√≠ch, t√≥m t·∫Øt n·ªôi dung vƒÉn b·∫£n."
        f"T√¥i s·∫Ω g·ª≠i cho b·∫°n ti√™u ƒë·ªÅ, ƒëo·∫°n tr√≠ch nh·ªè, link v√† n·ªôi dung (ph·∫ßn text trong trang web) c·ªßa m·ªôt trang web c√πng v·ªõi m·ªôt t·ª´ kh√≥a."
        f"B√¢y gi·ªù nhi·ªám v·ª• c·ªßa b·∫°n l√†: "
        f"H√£y d·ª±a v√†o nh·ªØng th√¥ng tin t√¥i g·ª≠i vi·∫øt m·ªôt b·∫£n t√≥m t·∫Øt b·∫±ng ti·∫øng Vi·ªát v·ªÅ n·ªôi dung c·ªßa trang web."
        f"Vi·∫øt ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu v√† ƒë·∫ßy ƒë·ªß √Ω ch√≠nh, tr√°nh lan man, t·ªëi ƒëa {num_words} ch·ªØ."
        f"Kh√¥ng t·ª± b·ªï sung th√¥ng tin kh√¥ng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong n·ªôi dung ƒë∆∞·ª£c giao.\n"
        f"Ti√™u ƒë·ªÅ trang web: {title} \n\n"
        f"ƒêo·∫°n tr√≠ch nh·ªè c·ªßa trang web: {snippet}\n\n"
        f"Link c·ªßa trang web: {link}\n\n"
        f"N·ªôi dung (ph·∫ßn text trong trang web) c·ªßa trang web: {content}\n\n"
        f"T·ª´ kh√≥a c·ªßa l·∫ßn n√†y: {key}\n\n"
    )   
    
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": model,
            "prompt": prompt,
            "stream": False
        }
    )
    
    if response.status_code == 200:
        return response.json()["response"]
    else:
        return f"L·ªói: {response.status_code} - {response.text}"
    
    
def is_related(title, snippet, link, model="gemma3:1b"):
    prompt = (
        f"T√¥i s·∫Ω g·ª≠i cho b·∫°n ti√™u ƒë·ªÅ, ƒëo·∫°n tr√≠ch nh·ªè v√† link c·ªßa m·ªôt trang web c√πng v·ªõi m·ªôt t·ª´ kh√≥a. B√¢y gi·ªù nhi·ªám v·ª• c·ªßa b·∫°n l√†: "
        f"Tr·∫£ l·ªùi ch·ªâ m·ªôt s·ªë duy nh·∫•t: \"1\" n·∫øu t·ª´ nh·ªØng th√¥ng tin v·ªÅ trang web m√† t√¥i g·ª≠i b·∫°n ƒë√°nh gi√° c√≥ n·ªôi dung li√™n quan nhi·ªÅu ƒë·∫øn t·ª´ kh√≥a, kh√¥ng li√™n quan th√¨ ghi \"0\", n·∫øu b·∫°n ph√¢n v√¢n v√† kh√¥ng quy·∫øt ƒë·ªãnh ƒë∆∞·ª£c th√¨ ghi \"2\"."
        f"Kh√¥ng th√™m b·∫•t k·ª≥ gi·∫£i th√≠ch hay b√¨nh lu·∫≠n n√†o kh√°c. R√µ r√†ng v√† ng·∫Øn g·ªçn.\n\n"
        f"H√£y c√¢n nh·∫Øc th·∫≠t k·ªπ b·ªüi v√¨ c√¢u tr·∫£ l·ªùi n√†y c·ªßa b·∫°n r·∫•t quan tr·ªçng."
        f"Ti√™u ƒë·ªÅ trang web: {title} \n\n"
        f"ƒêo·∫°n tr√≠ch nh·ªè c·ªßa trang web: {snippet}\n\n"
        f"Link c·ªßa trang web: {link}\n\n"
        f"T·ª´ kh√≥a c·ªßa l·∫ßn n√†y: {key}\n\n"
    )

    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": model,
            "prompt": prompt,
            "stream": False
        }
    )
    
    if response.status_code == 200:
        return response.json()["response"]
    else:
        return f"L·ªói: {response.status_code} - {response.text}"
  
    
def extract_info (text, model="gemma3:1b"):
    prompt = (
        f"B√¢y gi·ªù b·∫°n l√† m·ªôt AI c√≥ nhi·ªám v·ª• ƒë·ªçc hi·ªÉu n·ªôi dung c·ªßa m·ªôt trang web v√† tr√≠ch xu·∫•t ra c√°c th√¥ng tin c·ªët l√µi li√™n quan ƒë·∫øn t·ª´ kh√≥a ƒë∆∞·ª£c cung c·∫•p."
        f"Y√™u c·∫ßu:"
        f"- ƒê·ªçc n·ªôi dung ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ HTML c·ªßa m·ªôt trang web t√¥i s·∫Øp g·ª≠i d∆∞·ªõi ƒë√¢y. "
        f"- D·ª±a tr√™n t·ª´ kh√≥a ƒë∆∞·ª£c cung c·∫•p, tr√≠ch xu·∫•t ra nh·ªØng th√¥ng tin c√≥ li√™n quan tr·ª±c ti·∫øp ƒë·∫øn t·ª´ kh√≥a ƒë√≥, n·∫øu nh∆∞ kh√¥ng c√≥ th√¥ng tin li√™n quan, ghi ng·∫Øn g·ªçn \"Kh√¥ng c√≥ th√¥ng tin\"."
        f"- Kh√¥ng t·ª± b·ªï sung th√¥ng tin hay n√≥i v·ªÅ nh·ªØng th√¥ng tin kh√¥ng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong n·ªôi dung ƒë∆∞·ª£c giao."
        f"- N·∫øu c√≥ th·ªÉ, h√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát v√† tu√¢n theo c√°c m·ª•c sau:\n"
        f"    1. T√™n thi·∫øt b·ªã ho·∫∑c t√™n d√≤ng m√°y li√™n quan ƒë·∫øn t·ª´ kh√≥a, c√†ng nhi·ªÅu th√¥ng tin chi ti·∫øt c√†ng t·ªët."
        f"    2. T√™n c√¥ng c·ª• (tool) ho·∫∑c t√™n ph·∫ßn m·ªÅm ho·∫∑c ph∆∞∆°ng th·ª©c ƒë∆∞·ª£c d√πng ƒë·ªÉ th·ª±c hi·ªán"
        f"    3. C√°ch th·ª©c th·ª±c hi·ªán (h∆∞·ªõng d·∫´n ng·∫Øn g·ªçn n·∫øu c√≥)"
        f"    4. ƒêi·ªÅu ki·ªán c·∫ßn thi·∫øt ho·∫∑c l∆∞u √Ω khi th·ª±c hi·ªán"
        f"    5. B·∫•t k·ª≥ th√¥ng tin b·ªï sung h·ªØu √≠ch n√†o li√™n quan ƒë·∫øn t·ª´ kh√≥a"
        f"Ch·ªâ tr√≠ch xu·∫•t c√°c th√¥ng tin li√™n quan tr·ª±c ti·∫øp ƒë·∫øn t·ª´ kh√≥a. N·∫øu kh√¥ng c√≥ th√¥ng tin n√†o ph√π h·ª£p th√¨ h√£y tr·∫£ l·ªùi: \"Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan\"."
        f"T·ª´ kh√≥a c·ªßa l·∫ßn n√†y: {key}\n"
        f"N·ªôi dung text trong trang web m√† b·∫°n c·∫ßn x·ª≠ l√Ω: {text}"
    )
    
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": model,
            "prompt": prompt,
            "stream": False
        }
    )
    
    if response.status_code == 200:
        return response.json()["response"]
    else:
        return f"L·ªói: {response.status_code} - {response.text}"

def update_ai_response():
    for item in all_results:
        global cur_time
        print(f"üß† Update AI response cho: {item[TITLE]}")
        try:    
            item[SUMMARIZE] = strip_thoughts(summarize_text(item[TITLE], item[SNIPPET], item[LINK], item[CONTENT]))

            end_time = time.time()  # ghi l·∫°i th·ªùi gian k·∫øt th√∫c
            elapsed_time = end_time - cur_time
            cur_time = end_time
            print(f"‚è±Ô∏è Th·ªùi gian ch·∫°y summarize_text: {elapsed_time:.2f} gi√¢y")

            str_related = strip_thoughts(is_related(item[TITLE], item[SNIPPET], item[LINK]))
            end_time = time.time()  # ghi l·∫°i th·ªùi gian k·∫øt th√∫c
            elapsed_time = end_time - cur_time
            cur_time = end_time
            print(f"‚è±Ô∏è Th·ªùi gian ch·∫°y is_related: {elapsed_time:.2f} gi√¢y")

            try: 
                int_related = int(str_related)
            except ValueError: 
                int_related = 1
            if int_related==0: 
                item[IS_RELATED] = "Kh√¥ng"
            else: 
                item[IS_RELATED] = "C√≥"
                item[USEFUL_INFO] = strip_thoughts(extract_info(item[CONTENT]))
                end_time = time.time()  # ghi l·∫°i th·ªùi gian k·∫øt th√∫c
                elapsed_time = end_time - cur_time
                cur_time = end_time
                print(f"‚è±Ô∏è Th·ªùi gian ch·∫°y extract_info: {elapsed_time:.2f} gi√¢y")
            
        except Exception as e:
            print(f"[ERROR] ƒê√£ x·∫£y ra l·ªói: {e}")

update_ai_response()
end_time = time.time()  # ghi l·∫°i th·ªùi gian k·∫øt th√∫c
elapsed_time = end_time - cur_time
cur_time = end_time
print(f"‚è±Ô∏è Th·ªùi gian ch·∫°y update_ai_response: {elapsed_time:.2f} gi√¢y")
print(f"‚è±Ô∏è T·ªïng th·ªùi gian ch·∫°y: {end_time - start_time:.2f} gi√¢y")

####################################################################################### STORE

for item in all_results:
    if 'content' in item:
        del item['content']

try:
    today = datetime.today().date()

    # path = '..//output'
    path = '/home/huy/AAProjets/Scanning/AutoScanningToolv1/output'

    # Check if the directory exists
    if not os.path.exists(path):
        # Create the directory if it doesn't exist
        os.makedirs(path)
        print(f'The directory "{path}" has been created.')
        
    file_path = f'{path}//hacking_{today}.xlsx'
    sheet_name = f'google_{today}'

    df = pd.DataFrame(all_results)
    df = df.map(lambda x: str(x) if not isinstance(x, (int, float)) else x)

    # Check file exist, delete old sheet before add new sheet
    if os.path.exists(file_path):
        with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='new') as writer:
            if sheet_name in writer.book.sheetnames:
                writer.book.remove(writer.book[sheet_name])
            df.to_excel(writer, sheet_name=sheet_name, index=False)
    else:
        with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name=sheet_name, index=False)

    print(f'Exported {len(all_results)} data successful to {file_path} with sheet name {sheet_name}!')
except Exception as e:
    traceback.print_exc()
    print(f'Save data fail: {e}')